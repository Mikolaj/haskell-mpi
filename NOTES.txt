Implementation notes
--------------------

status_error
------------
Though Status has `status_error' field, it could be used with care:
"The error field in status is not needed for calls that return only
one status, such as MPI_WAIT, since that would only duplicate the
information returned by the function itself. The current design avoids
the additional overhead of setting it, in such cases. The field is
needed for calls that return multiple statuses, since each request may
have had a different failure." 
(quote from http://mpi-forum.org/docs/mpi22-report/node47.htm#Node47)

This means that, for example, during the call to MPI_Wait
implementation is free to leave this field filled with whatever
garbage got there during memory allocation. Haskell bindings are not
blanking out freshly allocated status for now either. So beware!

Test.Runner vs TestFramework
----------------------------
In order to be able to debug MPI bindings testsuite on a single-node
MPI installation one has to be able to separate output from processes
of different rank.

OpenMPI allows to do so via the --output-filename switch of mpirun,
but MPICH2 does not have similar feature. And since most of the output
in the testsuite is done from inside test harness library, there is
very little control over output.

Obvious solution would be to redirect stdout of the process to some
other file handle via dup2(2). However, there are several downsides:
1. Binding for dup2 (hDuplicateTo) is a GHC-only solutions
2. TestFramework does not play well with this solution, shutting
   output completely when stdout is redirected (probably "ncurses" is
   disappointed to find that output is not a terminal anymore)

Nevertheless, I decided to stick to hDuplicateTo and ditch
TestFramework in favor of TestRunner, since it allows for consistent
experience across MPI implementations.

Code coverage analysis
----------------------
It's very nice to have code coverage report for testsuite to make sure
that no major piece of code is left untested. However, current
profiling mechanism does not play well with MPI: when mpirun starts
two processes (on the single node), they both try to run to the same
.tix file at once. Mayhem ensues.

In order to fix this, Testsuite.hs has been made to depend on hpc
package, and after all tests has been run, HPC API is instructed to
write tix data to files rank<n>.tix.

Command line tool "hpc" could then be used to combine those into
single .tix file, which could be used to produce code coverage report.
Simple script "bin/coverage.sh" does all this automatically. Note:
script should be run from the toplevel project dir (where
bindings-mpi.cabal is residing).

Planned API coverage
--------------------
First tier:
     MPI_Send,
     MPI_Recv
     MPI_Bcast,
     MPI_Allgather
     MPI_Barrier
     MPI_Scatter,
     MPI_Gather
     MPI_Alltoall
     MPI_Isend,
     MPI_Wait, etc.
     MPI_Reduce_scatter
     MPI_Test,
     MPI_Cancel
     MPI_Wtime,
     MPI_Wtick
     MPI_Iprobe
     MPI_Ssend
     MPI_Probe

Second tier or under consideration:
     MPI_Op_create
     MPI_Pcontrol
     MPI_Type_commit
     MPI_Pack_external
     MPI_Register_datarep
     MPI_Op_free
     MPI_Query_thread
     MPI_Win_create,
     MPI_Put,
     MPI_Get,
     MPI_Accumulate
